<!DOCTYPE html>
<html lang="zh_CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0,viewport-fit=cover">
    <title>试了几周，在电脑上保留了3个大模型</title>
    <link rel="stylesheet" href="./assets/17462643395990.3286419337108403.css">
<link rel="stylesheet" href="./assets/17462643397560.46681296386005966.css">
<link rel="stylesheet" href="./assets/17462643398860.27100079076025696.css">
<link rel="stylesheet" href="./assets/17462643401840.36730068032591556.css">
<link rel="stylesheet" href="./assets/17462643411760.022284164477721524.css">
<link rel="stylesheet" href="./assets/17462643413150.49509598550083533.css">
<link rel="stylesheet" href="./assets/17462643414550.6029711818966502.css">
<link rel="stylesheet" href="./assets/17462643415980.3282914985472213.css">
<link rel="stylesheet" href="./assets/17462643417370.4859071607692653.css">

    <style>
        #page-content,
        #js_article_bottom_bar,
        .__page_content__ {
            max-width: 667px;
            margin: 0 auto;
        }
        img {
            max-width: 100%;
        }
        .sns_opr_btn::before {
            width: 16px;
            height: 16px;
            margin-right: 3px;
        }
    </style>
</head>
<body class="zh_CN wx_wap_page 

                                            wx_wap_desktop_fontsize_2    mm_appmsg
 comment_feature
 discuss_tab appmsg_skin_default appmsg_style_default ">
<div id="js_article" style="position:relative;" class="rich_media">
  
  
  
  <div id="js_base_container" class="rich_media_inner">
    
    
    
<div class="wx_row_immersive_stream_wrap" id="js_row_immersive_stream_wrap">
  
    <div id="js_row_immersive_cover_img">
    <img src="./assets/17462643390500.8684633729664023.jpeg" alt="cover_image" class="wx_follow_avatar_pic">
  </div>
    <div id="js_row_immersive_stream_mask" class="wx_row_immersive_stream_mask"></div>
</div>

    
    <div id="page-content" class="rich_media_area_primary" style="">
      <div class="rich_media_area_primary_inner">
        
        
        
                
        

        <div id="img-content" class="rich_media_wrp">
          
          <h1 class="rich_media_title " id="activity-name">
            
试了几周，在电脑上保留了3个大模型
          </h1>
          <div id="meta_content" class="rich_media_meta_list">
                                      <span id="copyright_logo" class="wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea">原创</span>
                                                      <span class="rich_media_meta rich_media_meta_text">
                                      Danglive
                                  </span>
                                      
                        <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
              <a href="javascript:void(0);" class="wx_tap_link js_wx_tap_highlight weui-wa-hotarea" id="js_name">
                Danglive              </a>
              
              <div id="js_profile_card"></div>
            </span>
            
            <span id="meta_content_hide_info" class="">
              <em id="publish_time" class="rich_media_meta rich_media_meta_text">2025年05月03日 16:30</em>
              <em id="js_ip_wording_wrp" class="rich_media_meta rich_media_meta_text" role="option" aria-labelledby="js_a11y_op_ip_wording js_ip_wording" style="display: inline-block;"><span id="js_a11y_op_ip_wording" aria-hidden="true"></span><span aria-hidden="true" id="js_ip_wording">上海</span></em>
              <em id="js_title_modify_wrp" class="rich_media_meta rich_media_meta_text" role="option" aria-labelledby="js_a11y_op_title_modify js_title_modify" style="display: none;"><span aria-hidden="true" id="js_title_modify"></span></em>
                            <span id="js_star" role="link" tabindex="0" class="wx_tap_link js_wx_tap_highlight rich_media_meta rich_media_meta_link rich_media_meta_star" style="display: none;"></span>
            </span>
                      </div>

          
          
          
            
              
              
            
              
              
                
              
            
          

          
          

          
                                        

          
                    

          
                              
                                        
                    
                    
          
          
          
          
          
                                                            <div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section><span leaf="">自从deepseek提供了本地部署LLM的可能之后，开始尝试通过ollama来进行本地部署。不得不说，ollama run qwen3:14b，这样的命令方式已经简单到家了。ollama直接下载模型文件，然后就能进入对话状态。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">我的电脑是2021年下半年自己组装的。当时正值显卡特别贵的一段时间，所以一开始用的是1060显卡，后来等了一段时间，换成了3080Ti 12GB的。现在也没有必要为了测试LLM而换新显卡，毕竟大显存的显卡还是挺贵的。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">好在大模型厂商们互相卷得厉害，非满血的模型对于消费级显卡上的部署越来越友好。最近主要试了本地部署几个模型：</span><span leaf=""><br></span><span leaf="">qwq:32b-preview-q4_K_M</span><span leaf=""><br></span><span leaf="">deepseek-r1:32b</span><span leaf=""><br></span><span leaf="">deepseek-r1:14b</span><span leaf=""><br></span><span leaf="">gemma3:27b</span><span leaf=""><br></span><span leaf="">gemma3:12b</span><span leaf=""><br></span><span leaf="">qwen3:14b</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">试下来，32b的模型，模型本身在20GB左右大小，在这个12GB的显卡上非常吃力，显存全部用满，且输出token速度实在是太慢了。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">所以保留了12b、14b这两个参数的，这种参数规模的模型文件，一般在9GB左右大小。分别是：</span><span leaf=""><br></span><span leaf="">deepseek-r1:14b &nbsp; 9.0GB</span><span leaf=""><br></span><span leaf="">gemma3:12b &nbsp; &nbsp; &nbsp; &nbsp; 8.1GB</span><span leaf=""><br></span><span leaf="">qwen3:14b &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;9.3GB</span><span leaf=""><br></span><span leaf=""><br></span><span leaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QX8497U9PKzj1LUsm2Uw0ur5kQEatxyVDkSSdECr5HgHTVoEazAVobU2R6dGiahcqh6zhVVJ3DWHPJaFAaKdlGQ/640?wx_fmt=png&amp;from=appmsg" class="rich_pages wxw-img js_insertlocalimg" data-ratio="0.6138888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000017" src="./assets/17462643392080.2901477842588346.png"></span></section><section><span leaf="">从运行速度来看，最快的是qwen3:14b，GPU利用率遥遥领先，可以达到90%以上。而且它是一个推理和非推理合一的模型，有特色。不需要推理的时候，在对话中加上“/no_think”即可，这样回答就干脆利落多了。在api调用时，虽然它依然会输出一对空的&lt;think&gt;&lt;/think&gt;标签，需要对返回字符串做一下处理，但它的回答已经算很容易被后续处理了。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">gemma3:12b，我也保留了，因为它是一个多模态的模型，可以对图片做解释。就是api调用的时候，有点烦它老是在回答中加点铺垫和总结，对于只要结果的程序调用（比如翻译一小段话），还需要在prompt中加些调教和限制。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">deepseek-r1:14b，也保留了。因为我在云端会调用deepseek的api，作为一个本地的参照，主要是可以时常查看一下线上线下两个版本的差异。它大概是最啰嗦的一个模型了，不过毕竟是中国之光，留着是一种情结。等r2出来了，估计会被替换成r2的某个版本。</span><span leaf=""><br></span><span leaf=""><br></span><span leaf="">电脑配置如下：</span><span leaf=""><br></span><span leaf=""><br></span><span leaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QX8497U9PKzj1LUsm2Uw0ur5kQEatxyVFVfHnlhdibhV4N5ljSStfIzSfnd1rtI3g6uqgCTq3egWfFYiaqKbpicaQ/640?wx_fmt=png&amp;from=appmsg" class="rich_pages wxw-img js_insertlocalimg" data-ratio="0.24722222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000018" src="./assets/17462643393380.8579404725261033.png"></span></section><section><span leaf="">处理器	AMD Ryzen 9 5950X 16-Core Processor &nbsp; &nbsp; &nbsp; &nbsp;3.40 GHz</span><span leaf=""><br></span><span leaf="">机带 RAM	128 GB &nbsp; &nbsp;2400MHz</span><span leaf=""><br></span><span leaf="">版本	Windows 11 专业版</span><span leaf=""><br></span><span leaf="">版本号	24H2</span><span leaf=""><br></span><span leaf="">显卡 &nbsp; &nbsp; &nbsp; &nbsp;NVIDIA GeForce RTX 3080 Ti &nbsp;12GB</span></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

          
        </div>
                        
                
        

        
        <div id="js_temp_bottom_area" class="rich_media_tool_area">
          
<div class="rich_media_tool__wrp">
    <div class="rich_media_tool">
    
                  <div class="rich_media_info weui-flex ">
      
            
            
          </div>
  </div>
  </div>

        </div>


                
              </div>
    </div>

    <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

    
    <div class="rich_media_area_extra">
      <div class="rich_media_area_extra_inner">
        
        <div id="page_bottom_area"></div>
      </div>
    </div>

    
    
  </div>

  
  



</div>
<div id="js_article_bottom_bar" class="bottom_bar_wrp">
      <div id="article_bottom_bar_area"></div>
    </div>

<!-- 评论数据 -->

</body>
</html>
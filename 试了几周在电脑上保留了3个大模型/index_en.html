html
<html lang="zh_CN">
 <head>
  <meta charset="utf-8"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0,viewport-fit=cover" name="viewport"/>
  <title>
   After trying for a few weeks, I kept three large models on the computer.
  </title>
  <link href="./assets/17462643395990.3286419337108403.css" rel="stylesheet"/>
  <link href="./assets/17462643397560.46681296386005966.css" rel="stylesheet"/>
  <link href="./assets/17462643398860.27100079076025696.css" rel="stylesheet"/>
  <link href="./assets/17462643401840.36730068032591556.css" rel="stylesheet"/>
  <link href="./assets/17462643411760.022284164477721524.css" rel="stylesheet"/>
  <link href="./assets/17462643413150.49509598550083533.css" rel="stylesheet"/>
  <link href="./assets/17462643414550.6029711818966502.css" rel="stylesheet"/>
  <link href="./assets/17462643415980.3282914985472213.css" rel="stylesheet"/>
  <link href="./assets/17462643417370.4859071607692653.css" rel="stylesheet"/>
  <style>
   #page-content,
        #js_article_bottom_bar,
        .__page_content__ {
            max-width: 667px;
            margin: 0 auto;
        }
        img {
            max-width: 100%;
        }
        .sns_opr_btn::before {
            width: 16px;
            height: 16px;
            margin-right: 3px;
        }
  </style>
 </head>
 <body class="zh_CN wx_wap_page wx_wap_desktop_fontsize_2 mm_appmsg comment_feature discuss_tab appmsg_skin_default appmsg_style_default">
  <div class="rich_media" id="js_article" style="position:relative;">
   <div class="rich_media_inner" id="js_base_container">
    <div class="wx_row_immersive_stream_wrap" id="js_row_immersive_stream_wrap">
     <div id="js_row_immersive_cover_img">
      <img alt="cover_image" class="wx_follow_avatar_pic" src="./assets/17462643390500.8684633729664023.jpeg"/>
     </div>
     <div class="wx_row_immersive_stream_mask" id="js_row_immersive_stream_mask">
     </div>
    </div>
    <div class="rich_media_area_primary" id="page-content" style="">
     <div class="rich_media_area_primary_inner">
      <div class="rich_media_wrp" id="img-content">
       <h1 class="rich_media_title" id="activity-name">
        After trying for a few weeks, I kept three large models on the computer.
       </h1>
       <div class="rich_media_meta_list" id="meta_content">
        <span class="wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea" id="copyright_logo">
         Original
        </span>
        <span class="rich_media_meta rich_media_meta_text">
         Danglive
        </span>
        <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
         <a class="wx_tap_link js_wx_tap_highlight weui-wa-hotarea" href="javascript:void(0);" id="js_name">
          Danglive
         </a>
         <div id="js_profile_card">
         </div>
        </span>
        <span class="" id="meta_content_hide_info">
         <em class="rich_media_meta rich_media_meta_text" id="publish_time">
          May 3, 2025 4:30 PM
         </em>
         <em aria-labelledby="js_a11y_op_ip_wording js_ip_wording" class="rich_media_meta rich_media_meta_text" id="js_ip_wording_wrp" role="option" style="display: inline-block;">
          <span aria-hidden="true" id="js_a11y_op_ip_wording">
          </span>
          <span aria-hidden="true" id="js_ip_wording">
           Shanghai
          </span>
         </em>
         <em aria-labelledby="js_a11y_op_title_modify js_title_modify" class="rich_media_meta rich_media_meta_text" id="js_title_modify_wrp" role="option" style="display: none;">
          <span aria-hidden="true" id="js_title_modify">
          </span>
         </em>
         <span class="wx_tap_link js_wx_tap_highlight rich_media_meta rich_media_meta_link rich_media_meta_star" id="js_star" role="link" style="display: none;" tabindex="0">
         </span>
        </span>
       </div>
       <div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content">
        <section>
         <span leaf="">
          Since DeepSeek made it possible to deploy LLMs locally, I started trying to deploy them locally using Ollama. I have to say, commands like "ollama run qwen3:14b" are now incredibly simple. Ollama directly downloads the model files, and then you can immediately enter a conversation state.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          My computer was self-assembled in the second half of 2021. At that time, it was a period when graphics cards were especially expensive, so I initially used a 1060 GPU. After waiting for a while, I upgraded to a 3080 Ti 12GB. Now, there's no need to replace the graphics card for testing LLMs, since high-memory GPUs are still quite expensive.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Fortunately, the competition among large model providers is intense, making non-full-blooded models increasingly friendly for deployment on consumer-grade GPUs. Recently, I have mainly tried deploying several models locally:
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          qwq:32b-preview-q4_K_M
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          deepseek-r1:32b
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          deepseek-r1:14b
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          gemma3:27b
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          gemma3:12b
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Qwen3:14B
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          After trying it out, the 32b model itself is about 20GB in size, which is very demanding on a 12GB GPU. It completely fills up the GPU memory, and the output token speed is simply too slow.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Therefore, the models that retain parameters 12b and 14b generally have a file size of around 9GB. They are respectively:
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          deepseek-r1:14b   9.0GB
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          gemma3:12b         8.1GB
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          qwen3:14b            9.3GB
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100000017" data-ratio="0.6138888888888889" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QX8497U9PKzj1LUsm2Uw0ur5kQEatxyVDkSSdECr5HgHTVoEazAVobU2R6dGiahcqh6zhVVJ3DWHPJaFAaKdlGQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="./assets/17462643392080.2901477842588346.png" type="block"/>
         </span>
        </section>
        <section>
         <span leaf="">
          In terms of running speed, the fastest model is qwen3:14b, which significantly outperforms others in GPU utilization, reaching over 90%. Additionally, it is a unified model for both inference and non-inference tasks, offering a unique feature. When inference is not needed, simply adding "/no_think" in the conversation results in much more concise and direct responses. When calling the API, although it still outputs an empty pair of "thinking" tags, some processing of the returned string is required, its responses are already quite easy to handle in subsequent processing.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          gemma3:12b, I also kept it because it is a multimodal model capable of interpreting images. However, it's a bit annoying when using the API, as it often adds some introductory remarks and summaries in its responses. For programmatic calls that only require results (such as translating a short passage), some tuning and restrictions need to be added to the prompt.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          deepseek-r1:14b is also retained. Because I will call the DeepSeek API in the cloud, using it as a local reference, mainly to check the differences between the online and offline versions from time to time. It's probably the most verbose model, but since it's a pride of China, keeping it is more of an emotional attachment. Once r2 is released, it's likely to be replaced by a version of r2.
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          The computer configuration is as follows:
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          <img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100000018" data-ratio="0.24722222222222223" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QX8497U9PKzj1LUsm2Uw0ur5kQEatxyVFVfHnlhdibhV4N5ljSStfIzSfnd1rtI3g6uqgCTq3egWfFYiaqKbpicaQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="./assets/17462643393380.8579404725261033.png" type="block"/>
         </span>
        </section>
        <section>
         <span leaf="">
          Processor	AMD Ryzen 9 5950X 16-Core Processor 	3.40 GHz
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Onboard RAM 128 GB    2400MHz
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Version	Windows 11 Professional
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Version Number	24H2
         </span>
         <span leaf="">
          <br/>
         </span>
         <span leaf="">
          Graphics Card        NVIDIA GeForce RTX 3080 Ti  12GB
         </span>
        </section>
        <p style="display: none;">
         <mp-style-type data-value="3">
         </mp-style-type>
        </p>
       </div>
      </div>
      <div class="rich_media_tool_area" id="js_temp_bottom_area">
       <div class="rich_media_tool__wrp">
        <div class="rich_media_tool">
         <div class="rich_media_info weui-flex">
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
    <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none">
    </div>
    <div class="rich_media_area_extra">
     <div class="rich_media_area_extra_inner">
      <div id="page_bottom_area">
      </div>
     </div>
    </div>
   </div>
  </div>
  <div class="bottom_bar_wrp" id="js_article_bottom_bar">
   <div id="article_bottom_bar_area">
   </div>
  </div>
  Comment data
 </body>
</html>
